{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In this section, we are building a tokenizer. To do that, we want to:\n",
    "\n",
    "    - create a list of words by spliting text by whitespace\n",
    "    - make all words lower case\n",
    "    - filter out rare words, that occurred less than N times in the corpus\n",
    "    - setting the vocab length to the total number of unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "text_file = \"../data/wiki_text_data.txt\"\n",
    "\n",
    "with open(text_file, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    # remove punctuation and non alphabetic characters \n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
    "    lower_case_words = remove_punctuation.lower()\n",
    "    words = lower_case_words.split(' ')\n",
    "\n",
    "    # print count of words in split_words_by_whitespace\n",
    "    print(f\"Number of words before filtering: {len(words)}\")\n",
    "\n",
    "    # get word counts \n",
    "\n",
    "    top_k = 30000\n",
    "    word_counts = Counter(words)\n",
    "    top_words = dict(word_counts.most_common(top_k))\n",
    "    word_to_id = {word: i for i, word in enumerate(top_words.keys())}\n",
    "    id_to_word = {i: word for i, word in enumerate(top_words.keys())}\n",
    "\n",
    "    # Sum their counts\n",
    "    total_count = sum(count for word, count in top_words.items())\n",
    "\n",
    "    print(f\"Total count of top {top_k} words: {total_count}\")\n",
    "    # Optional: Show what percentage of all words this represents\n",
    "    total_words = sum(word_counts.values())\n",
    "    percentage = (total_count / total_words) * 100\n",
    "    print(f\"This represents {percentage:.2f}% of all words in the corpus\")\n",
    "\n",
    "    # filter corpus to only include words in the tok k words\n",
    "    corpus = [word for word in words if word in top_words]\n",
    "    print(\"corpus length:\", len(corpus))\n",
    "\n",
    "    return word_to_id, id_to_word, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before filtering: 17005208\n",
      "Total count of top 30000 words: 16315126\n",
      "This represents 95.94% of all words in the corpus\n",
      "corpus length: 16315126\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "word_to_id, id_to_word, corpus = tokenizer(text)\n",
    "\n",
    "#print(corpus[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data from the corpus\n",
    "# The training data looks like a list of tuples, \n",
    "# where each tuple contains a list of context words and the target word (not the IDs)\n",
    "\n",
    "def generate_training_data(corpus):\n",
    "    data = []\n",
    "\n",
    "    # start from index 2 and end 2 positions before the last word\n",
    "    # this ensures we always have 2 words before and after the target word\n",
    "    # for a 5-len sliding window\n",
    "\n",
    "    for i in range(2, len(corpus) - 2):\n",
    "        # Get the context words\n",
    "        # 'i' is the index of the target word\n",
    "        # [i-2:i] gets the two words before the target word\n",
    "        # [i+1:i+3] gets the two words after the target word\n",
    "        context_words = corpus[i-2:i] + corpus[i+1:i+3]\n",
    "        \n",
    "        # Get the target word\n",
    "        target_word = corpus[i]\n",
    "\n",
    "        # Append the tuple to the data list\n",
    "        data.append((context_words, target_word))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW training data generated\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "training_data = generate_training_data(corpus)\n",
    "print(\"CBOW training data generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training data, word to id mappings, id to word mappings, and corpus to pytorch tensors\n",
    "import torch\n",
    "\n",
    "# save the training data\n",
    "torch.save(training_data, \"../data/eve_training_data.pt\")\n",
    "\n",
    "# save the word to id mappings\n",
    "torch.save(word_to_id, \"../data/eve_word_to_id.pt\")\n",
    "\n",
    "# save the id to word mappings\n",
    "torch.save(id_to_word, \"../data/eve_id_to_word.pt\")\n",
    "\n",
    "# save the corpus\n",
    "torch.save(corpus, \"../data/eve_corpus.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16315122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
