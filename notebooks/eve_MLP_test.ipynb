{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Download the embeddings from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevelyntants\u001b[0m (\u001b[33mbryars-bryars\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/predicting_hackernews_upvotes/notebooks/wandb/run-20250418_111850-l88ks8sg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks/runs/l88ks8sg' target=\"_blank\">happy-night-9</a></strong> to <a href='https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks' target=\"_blank\">https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks/runs/l88ks8sg' target=\"_blank\">https://wandb.ai/bryars-bryars/predicting_hackernews_upvotes-notebooks/runs/l88ks8sg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings downloaded to: temp_data\n",
      "Model weights downloaded to: temp_data\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "# Use the embeddings artifact\n",
    "embeddings_artifact = run.use_artifact('bryars-bryars/cbow-wiki/embeddings:v4', type='embeddings')\n",
    "embeddings_dir = embeddings_artifact.download(root='temp_data')\n",
    "\n",
    "# Use the model weights artifact\n",
    "model_artifact = run.use_artifact('bryars-bryars/cbow-wiki/model-weights:v6', type='model')\n",
    "model_dir = model_artifact.download(root='temp_data')\n",
    "\n",
    "# Now you have both directories\n",
    "print(f\"Embeddings downloaded to: {embeddings_dir}\")\n",
    "print(f\"Model weights downloaded to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset and Data Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4000\n",
      "GPU Memory: 16.78 GB\n"
     ]
    }
   ],
   "source": [
    "# Check that the GPU is being used\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "else: \n",
    "   print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a class for the dataset\n",
    "# The dataset is a csv file with two columns: title and score\n",
    "# Drop NA values\n",
    "# Tokenize the titles and convert them to embeddings using the CBOW model embeddings\n",
    "# return titles and scores\n",
    "class HackerNewsDataset(Dataset):\n",
    "    def __init__(self, csv_file, word_to_idx, embedding_file):\n",
    "        df = pd.read_csv(csv_file).dropna(subset=[\"title\", \"score\"])\n",
    "        self.titles = df[\"title\"].tolist()\n",
    "        self.scores = torch.tensor(df[\"score\"].values, dtype=torch.float32)\n",
    "        \n",
    "        # load word2idx and embeddings\n",
    "        self.word_to_idx = torch.load(word_to_idx)\n",
    "        print(\"word to id type is :\" ,type(self.word_to_idx))\n",
    "\n",
    "        # The embeddings are saved as a state dict, so we need to load them as a state dict\n",
    "        state_dict = torch.load(embedding_file)\n",
    "        # To get the actual embeddings tensor\n",
    "        if isinstance(state_dict, dict):\n",
    "            self.embeddings = state_dict['weight'] if 'weight' in state_dict else next(iter(state_dict.values()))\n",
    "        else:\n",
    "            self.embeddings = state_dict\n",
    "        print(f\"Embeddings shape: {self.embeddings.shape}\")\n",
    "    \n",
    "    def _preprocess_title(self, title):\n",
    "        tokens = title.lower().split()\n",
    "        word_ids = [self.word_to_idx[word] for word in tokens if word in self.word_to_idx]\n",
    "        return word_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        score = self.scores[idx]\n",
    "    \n",
    "        word_ids = self._preprocess_title(title)\n",
    "\n",
    "        if len(word_ids) == 0:\n",
    "            title_embedding = torch.zeros(self.embeddings.shape[1])\n",
    "        else:\n",
    "            word_embeddings = self.embeddings[word_ids]  # shape: (num_words, embedding_dim)\n",
    "            title_embedding = word_embeddings.mean(dim=0)\n",
    "    \n",
    "        #return title, score, word_ids, word_embeddings, title_embedding\n",
    "        return title_embedding, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data, word2idx and embeddings\n",
    "data_url = \"https://huggingface.co/datasets/danbhf/hackernews_title_training/resolve/main/hn_title_training_notnorm_2008_2024.csv\"\n",
    "embeddings_path = \"./temp_data/embeddings_epoch5_2025_04_17__15_28_43.pt\"\n",
    "word2idx_path = \"./temp_data/word_to_id.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3338/735024560.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.word_to_idx = torch.load(word_to_idx)\n",
      "/tmp/ipykernel_3338/735024560.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(embedding_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to id type is : <class 'dict'>\n",
      "Embeddings shape: torch.Size([30000, 200])\n"
     ]
    }
   ],
   "source": [
    "dataset = HackerNewsDataset(data_url, word2idx_path, embeddings_path)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=256,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=4     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=256,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Roboflow (YC S20) is hiring a new leader for its machine learning team\n",
      "Score: 1.0\n",
      "Word IDs: [10, 15827, 5, 57, 662, 13, 44, 798, 1778, 360]\n",
      "Word Embeddings: tensor([[ 0.5983, -0.0779,  0.0396,  ..., -0.0844, -0.1280, -0.0292],\n",
      "        [-0.0269,  0.0160, -0.0012,  ...,  0.0208, -0.0325, -0.0226],\n",
      "        [ 0.1236, -0.0196, -0.0173,  ...,  0.0365,  0.0887, -0.0424],\n",
      "        ...,\n",
      "        [ 0.0406, -0.2087, -0.0118,  ..., -0.0579, -0.0404,  0.0680],\n",
      "        [ 0.0048, -0.0985, -0.0934,  ..., -0.1194, -0.1043, -0.0247],\n",
      "        [ 0.0165, -0.0742, -0.0102,  ..., -0.0096, -0.0446, -0.0444]])\n",
      "Title Embedding: tensor([ 2.2575e-01,  2.2184e-02, -3.8372e-03,  1.6167e-02, -4.3651e-02,\n",
      "         1.1591e-02,  1.6441e-01, -4.4540e-02, -1.2221e-01, -1.5744e-01,\n",
      "         1.4680e-01,  7.6242e-02, -5.2974e-02,  1.0277e-02,  3.7014e-02,\n",
      "        -2.4790e-02, -3.9760e-02,  5.8116e-02, -3.9913e-01,  2.4221e-01,\n",
      "        -5.1476e-03, -2.8277e-02,  8.9372e-02,  2.2611e+00, -4.8176e-02,\n",
      "         8.8180e-03, -1.7317e-01, -1.3460e-01,  5.8148e-02, -3.3056e-01,\n",
      "         1.6889e-01, -5.2200e-01, -2.1502e-01,  4.0033e-02,  6.9281e-02,\n",
      "         1.1023e-01, -2.4927e-03,  1.8077e-01,  4.5946e-02, -8.4187e-04,\n",
      "        -3.3083e-04, -1.1124e-01,  1.3250e-02,  2.2491e-02, -3.5156e-02,\n",
      "        -1.2038e-01, -4.7239e-02,  1.5135e-02,  4.8740e-02, -8.5061e-02,\n",
      "        -1.5205e-01,  1.4815e-02, -4.8137e-02,  3.7362e-02,  2.0136e-02,\n",
      "        -2.1726e-02,  4.6387e-02, -4.1079e-02,  6.1998e-02,  9.7577e-02,\n",
      "        -4.4561e-04,  6.1541e-02,  1.1369e-01, -2.8579e-01, -5.0246e-02,\n",
      "        -1.9947e-01,  3.4275e-02, -2.1848e-02,  4.9564e-02, -9.1063e-02,\n",
      "         2.7212e-02,  1.0131e-01,  3.2560e-02,  1.2004e-02, -3.2875e-02,\n",
      "         1.6777e-02, -1.7560e-02,  5.7761e-02,  8.7832e-01, -5.4513e-03,\n",
      "        -8.3865e-02,  7.1678e-01,  1.2069e-01,  2.1404e-02,  1.9971e-02,\n",
      "         1.7691e-02,  1.0736e-02, -1.0599e-01,  5.6616e-02,  1.8534e-02,\n",
      "        -4.9738e-02, -2.7110e-02,  5.1406e-02,  7.5024e-03,  2.5349e-02,\n",
      "        -3.3351e-01,  9.7177e-01,  3.2362e-01,  5.2520e-02,  6.8069e-02,\n",
      "        -2.7058e-02,  1.6563e-01,  2.7285e-02,  1.0195e-01, -1.9542e-02,\n",
      "        -3.3202e-02, -1.7100e-01, -7.4051e-02,  3.6271e-02, -2.0125e-01,\n",
      "         1.0428e-01, -9.7988e-03, -1.0776e-02,  1.9804e-03,  2.3351e-01,\n",
      "        -4.6339e-02,  1.0331e-01,  6.2124e-01, -1.1646e-01,  1.3541e-01,\n",
      "        -3.1005e-02, -7.0878e-02,  3.0396e-02, -2.9146e-02,  2.1303e-02,\n",
      "        -5.9476e-02,  1.8708e-02,  1.1889e-02,  2.5505e-02, -5.9104e-02,\n",
      "         1.7676e-02,  1.8442e-02,  1.5095e-01, -8.3100e-02,  1.5026e-01,\n",
      "        -9.2144e-02,  6.8172e-02, -5.3693e-02,  1.1833e-01,  3.3521e-02,\n",
      "         1.1301e-01,  4.9193e-02,  2.2009e-02, -5.3329e-02, -7.9024e-02,\n",
      "         1.7132e-01,  2.3184e-02,  2.2313e-03, -6.8182e-03,  9.3772e-02,\n",
      "        -1.1643e-01,  1.6366e-01,  6.2570e-02, -2.2628e-02,  3.3990e-02,\n",
      "         5.1375e-02,  5.7076e-02,  5.9299e-03, -7.6409e-01, -2.2729e-02,\n",
      "        -2.7655e-02,  5.6674e-03, -6.8194e-02,  1.4856e-01,  3.2367e-02,\n",
      "        -1.7577e-02, -9.7410e-02, -7.4705e-02, -6.8471e-02, -7.4005e-02,\n",
      "        -1.1676e-01, -7.8488e-02, -8.2099e-02, -1.3002e-02,  9.0254e-02,\n",
      "         1.8762e-01, -4.3408e-02, -5.1054e-03, -1.0735e-01,  1.0432e-01,\n",
      "        -2.5955e-02,  4.1700e-02, -6.9799e-02,  2.2662e-02,  1.3214e-01,\n",
      "         7.2960e-02, -5.2402e-02, -4.0574e-02,  1.6371e-02, -9.7329e-02,\n",
      "         4.8071e-02,  1.9443e-02, -2.5737e-02, -7.4410e-02, -6.8029e-02,\n",
      "        -8.3966e-02, -3.7496e-02,  5.8074e-03, -7.6887e-02, -3.1286e-02])\n",
      "Title embedding length is : 200\n"
     ]
    }
   ],
   "source": [
    "# to use this, add the word_ids, word_embeddings, title_embedding to the dataset return\n",
    "# Sanity check a sample\n",
    "title, score, word_ids, word_embeddings, title_embedding = dataset[0]\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"Word IDs: {word_ids}\")\n",
    "print(f\"Word Embeddings: {word_embeddings}\")\n",
    "print(f\"Title Embedding: {title_embedding}\")\n",
    "print(\"Title embedding length is :\", len(title_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MLP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a MLP regressor\n",
    "class Regressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=200, out_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=128, out_features=64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=32, out_features=16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=16, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        out = self.seq(inpt)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training:  18%|█▊        | 2695/14815 [03:04<15:10, 13.31it/s, train_loss=13.1]"
     ]
    }
   ],
   "source": [
    "# Setup regressor\n",
    "mReg = Regressor()\n",
    "optimizer = torch.optim.Adam(mReg.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    # Training phase\n",
    "    mReg.train()\n",
    "    train_losses = []\n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch} Training')\n",
    "    \n",
    "    for title_embedding, score in train_bar:\n",
    "        # Forward pass\n",
    "        out = mReg(title_embedding)\n",
    "        loss = torch.nn.functional.l1_loss(out.squeeze(), score)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store loss\n",
    "        train_losses.append(loss.item())\n",
    "        train_bar.set_postfix({'train_loss': np.mean(train_losses[-100:])})\n",
    "    \n",
    "    # Calculate average train loss for the epoch\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    \n",
    "    # Testing phase\n",
    "    mReg.eval()\n",
    "    test_losses = []\n",
    "    test_bar = tqdm(test_loader, desc=f'Epoch {epoch} Testing')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for title_embedding, score in test_bar:\n",
    "            # Forward pass\n",
    "            out = mReg(title_embedding)\n",
    "            loss = torch.nn.functional.l1_loss(out.squeeze(), score)\n",
    "            \n",
    "            # Store loss\n",
    "            test_losses.append(loss.item())\n",
    "            test_bar.set_postfix({'test_loss': np.mean(test_losses[-100:])})\n",
    "    \n",
    "    # Calculate average test loss for the epoch\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': avg_test_loss\n",
    "    })\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'\\nEpoch {epoch}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
