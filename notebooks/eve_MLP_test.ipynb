{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Download the embeddings from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "# Use the embeddings artifact\n",
    "embeddings_artifact = run.use_artifact('bryars-bryars/cbow-wiki/embeddings:v4', type='embeddings')\n",
    "embeddings_dir = embeddings_artifact.download(root='temp_data')\n",
    "\n",
    "# Use the model weights artifact\n",
    "model_artifact = run.use_artifact('bryars-bryars/cbow-wiki/model-weights:v6', type='model')\n",
    "model_dir = model_artifact.download(root='temp_data')\n",
    "\n",
    "# Now you have both directories\n",
    "print(f\"Embeddings downloaded to: {embeddings_dir}\")\n",
    "print(f\"Model weights downloaded to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset and Data Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4000\n",
      "GPU Memory: 16.78 GB\n"
     ]
    }
   ],
   "source": [
    "# Check that the GPU is being used\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "else: \n",
    "   print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# First set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a class for the dataset\n",
    "# The dataset is a csv file with two columns: title and score\n",
    "# Drop NA values\n",
    "# Tokenize the titles and convert them to embeddings using the CBOW model embeddings\n",
    "# return titles and scores\n",
    "class HackerNewsDataset(Dataset):\n",
    "    def __init__(self, csv_file, word_to_idx, embedding_file):\n",
    "        df = pd.read_csv(csv_file).dropna(subset=[\"title\", \"score\"])\n",
    "        self.titles = df[\"title\"].tolist()\n",
    "        self.scores = torch.tensor(df[\"score\"].values, dtype=torch.float32)\n",
    "        \n",
    "        # load word2idx and embeddings\n",
    "        self.word_to_idx = torch.load(word_to_idx)\n",
    "        print(\"word to id type is :\" ,type(self.word_to_idx))\n",
    "\n",
    "        # The embeddings are saved as a state dict, so we need to load them as a state dict\n",
    "        state_dict = torch.load(embedding_file)\n",
    "        # To get the actual embeddings tensor\n",
    "        if isinstance(state_dict, dict):\n",
    "            self.embeddings = state_dict['weight'] if 'weight' in state_dict else next(iter(state_dict.values()))\n",
    "        else:\n",
    "            self.embeddings = state_dict\n",
    "        print(f\"Embeddings shape: {self.embeddings.shape}\")\n",
    "    \n",
    "    def _preprocess_title(self, title):\n",
    "        tokens = title.lower().split()\n",
    "        word_ids = [self.word_to_idx[word] for word in tokens if word in self.word_to_idx]\n",
    "        return word_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        score = self.scores[idx]\n",
    "    \n",
    "        word_ids = self._preprocess_title(title)\n",
    "\n",
    "        if len(word_ids) == 0:\n",
    "            title_embedding = torch.zeros(self.embeddings.shape[1])\n",
    "        else:\n",
    "            word_embeddings = self.embeddings[word_ids]  # shape: (num_words, embedding_dim)\n",
    "            title_embedding = word_embeddings.mean(dim=0)\n",
    "    \n",
    "        #return title, score, word_ids, word_embeddings, title_embedding\n",
    "        return title_embedding, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data, word2idx and embeddings\n",
    "data_url = \"https://huggingface.co/datasets/danbhf/hackernews_title_training/resolve/main/hn_title_training_notnorm_2008_2024.csv\"\n",
    "embeddings_path = \"./temp_data/embeddings_epoch5_2025_04_17__15_28_43.pt\"\n",
    "word2idx_path = \"./temp_data/word_to_id.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182212/735024560.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.word_to_idx = torch.load(word_to_idx)\n",
      "/tmp/ipykernel_182212/735024560.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(embedding_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to id type is : <class 'dict'>\n",
      "Embeddings shape: torch.Size([30000, 200])\n"
     ]
    }
   ],
   "source": [
    "dataset = HackerNewsDataset(data_url, word2idx_path, embeddings_path)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=512,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=4     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=512,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use this, add the word_ids, word_embeddings, title_embedding to the dataset return\n",
    "# Sanity check a sample\n",
    "title, score, word_ids, word_embeddings, title_embedding = dataset[0]\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"Word IDs: {word_ids}\")\n",
    "print(f\"Word Embeddings: {word_embeddings}\")\n",
    "print(f\"Title Embedding: {title_embedding}\")\n",
    "print(\"Title embedding length is :\", len(title_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MLP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a MLP regressor\n",
    "class Regressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=200, out_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=128, out_features=64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=32, out_features=16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=16, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        out = self.seq(inpt)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training:   0%|          | 0/7408 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training:   7%|▋         | 553/7408 [00:10<02:04, 55.18it/s, train_loss=13.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title_embedding, score \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     title_embedding \u001b[38;5;241m=\u001b[39m title_embedding\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     score \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/predicting_hackernews_upvotes/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/predicting_hackernews_upvotes/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/predicting_hackernews_upvotes/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/predicting_hackernews_upvotes/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/predicting_hackernews_upvotes/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup regressor\n",
    "mReg = Regressor().to(device)\n",
    "optimizer = torch.optim.Adam(mReg.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    # Training phase\n",
    "    mReg.train()\n",
    "    train_losses = []\n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch} Training')\n",
    "    \n",
    "    for title_embedding, score in train_bar:\n",
    "        # Move data to device\n",
    "        title_embedding = title_embedding.to(device)\n",
    "        score = score.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = mReg(title_embedding)\n",
    "        loss = torch.nn.functional.l1_loss(out.squeeze(), score)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store loss\n",
    "        train_losses.append(loss.item())\n",
    "        train_bar.set_postfix({'train_loss': np.mean(train_losses[-100:])})\n",
    "    \n",
    "    # Calculate average train loss for the epoch\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    \n",
    "    # Testing phase\n",
    "    mReg.eval()\n",
    "    test_losses = []\n",
    "    test_bar = tqdm(test_loader, desc=f'Epoch {epoch} Testing')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for title_embedding, score in test_bar:\n",
    "            # Move data to device\n",
    "            title_embedding = title_embedding.to(device)\n",
    "            score = score.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = mReg(title_embedding)\n",
    "            loss = torch.nn.functional.l1_loss(out.squeeze(), score)\n",
    "            \n",
    "            # Store loss\n",
    "            test_losses.append(loss.item())\n",
    "            test_bar.set_postfix({'test_loss': np.mean(test_losses[-100:])})\n",
    "    \n",
    "    # Calculate average test loss for the epoch\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': avg_test_loss\n",
    "    })\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'\\nEpoch {epoch}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
