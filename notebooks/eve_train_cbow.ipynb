{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "wiki_text_file = \"../data/wiki_text_data.txt\"\n",
    "\n",
    "\n",
    "with open(wiki_text_file, \"r\") as file:\n",
    "   text = file.read()\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "  \n",
    "   # remove punctuation and non alphabetic characters\n",
    "   remove_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
    "   lower_case_words = remove_punctuation.lower()\n",
    "   words = lower_case_words.split(' ')\n",
    "\n",
    "\n",
    "   # print count of words in split_words_by_whitespace\n",
    "   print(f\"Number of words before filtering: {len(words)}\")\n",
    "\n",
    "\n",
    "   # get word counts\n",
    "\n",
    "\n",
    "   top_k = 30000\n",
    "   word_counts = Counter(words)\n",
    "   top_words = dict(word_counts.most_common(top_k))\n",
    "   word_to_id = {word: i for i, word in enumerate(top_words.keys())}\n",
    "   id_to_word = {i: word for i, word in enumerate(top_words.keys())}\n",
    "\n",
    "\n",
    "   # Sum their counts\n",
    "   total_count = sum(count for word, count in top_words.items())\n",
    "\n",
    "\n",
    "   print(f\"Total count of top {top_k} words: {total_count}\")\n",
    "   # Optional: Show what percentage of all words this represents\n",
    "   total_words = sum(word_counts.values())\n",
    "   percentage = (total_count / total_words) * 100\n",
    "   print(f\"This represents {percentage:.2f}% of all words in the corpus\")\n",
    "\n",
    "\n",
    "   # filter corpus to only include words in the tok k words\n",
    "   corpus = [word for word in words if word in top_words]\n",
    "   print(\"corpus length:\", len(corpus))\n",
    "\n",
    "\n",
    "   return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before filtering: 17005208\n",
      "Total count of top 30000 words: 16315126\n",
      "This represents 95.94% of all words in the corpus\n",
      "corpus length: 16315126\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "word_to_id, id_to_word, corpus = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#print(corpus[:100])\n",
    "\n",
    "# Generate the training data from the corpus\n",
    "# The training data looks like a list of tuples,\n",
    "# where each tuple contains a list of context words and the target word (not the IDs)\n",
    "\n",
    "\n",
    "def generate_training_data(corpus):\n",
    "   data = []\n",
    "\n",
    "\n",
    "   # start from index 2 and end 2 positions before the last word\n",
    "   # this ensures we always have 2 words before and after the target word\n",
    "   # for a 5-len sliding window\n",
    "\n",
    "\n",
    "   for i in range(2, len(corpus) - 2):\n",
    "       # Get the context words\n",
    "       # 'i' is the index of the target word\n",
    "       # [i-2:i] gets the two words before the target word\n",
    "       # [i+1:i+3] gets the two words after the target word\n",
    "       context_words = corpus[i-2:i] + corpus[i+1:i+3]\n",
    "      \n",
    "       # Get the target word\n",
    "       target_word = corpus[i]\n",
    "\n",
    "\n",
    "       # Append the tuple to the data list\n",
    "       data.append((context_words, target_word))\n",
    "\n",
    "\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW training data generated\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "training_data = generate_training_data(corpus)\n",
    "print(\"CBOW training data generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevelyntants\u001b[0m (\u001b[33mbryars-bryars\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/predicting_hackernews_upvotes/notebooks/wandb/run-20250417_143912-hbcqzx6d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bryars-bryars/cbow-wiki/runs/hbcqzx6d' target=\"_blank\">stellar-snowball-7</a></strong> to <a href='https://wandb.ai/bryars-bryars/cbow-wiki' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bryars-bryars/cbow-wiki' target=\"_blank\">https://wandb.ai/bryars-bryars/cbow-wiki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bryars-bryars/cbow-wiki/runs/hbcqzx6d' target=\"_blank\">https://wandb.ai/bryars-bryars/cbow-wiki/runs/hbcqzx6d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bryars-bryars/cbow-wiki/runs/hbcqzx6d?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x70a975a2c4c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb with your configuration\n",
    "wandb.init(\n",
    "   project=\"cbow-wiki\",\n",
    "   config={\n",
    "       # Model parameters\n",
    "       \"embedding_dim\": 200,\n",
    "       \"vocab_size\": 30000,\n",
    "      \n",
    "       # Training parameters\n",
    "       \"batch_size\": 128,\n",
    "       \"learning_rate\": 0.001,\n",
    "       \"num_epochs\": 5,\n",
    "       \"train_split\": 0.7,\n",
    "      \n",
    "       # Optimizer parameters\n",
    "       \"weight_decay\": 1e-5,\n",
    "      \n",
    "       # DataLoader parameters\n",
    "       \"num_workers\": 4,\n",
    "      \n",
    "       # Architecture details\n",
    "       \"model_type\": \"CBOW\",\n",
    "       \"context_size\": 4  # 2 words before + 2 words after\n",
    "   }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use the config values throughout your code\n",
    "EMBEDDING_DIM = wandb.config.embedding_dim\n",
    "BATCH_SIZE = wandb.config.batch_size\n",
    "LEARNING_RATE = wandb.config.learning_rate\n",
    "NUM_EPOCHS = wandb.config.num_epochs\n",
    "TRAIN_SPLIT = wandb.config.train_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "GPU Memory: 10.39 GB\n"
     ]
    }
   ],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "   def __init__(self, data, word_to_id):\n",
    "       self.data = data\n",
    "       self.word_to_id = word_to_id\n",
    "\n",
    "\n",
    "   # overriding the __len__ method to tell PyTorch how many samples you have\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "\n",
    "\n",
    "   # overriding the __getitem__ method\n",
    "   # to tell PyTorch how to retrieve a specific sample and convert it to the format your model expects\n",
    "   def __getitem__(self, idx):\n",
    "       context, target = self.data[idx]\n",
    "       context_ids = torch.tensor([self.word_to_id[word] for word in context], dtype=torch.long)\n",
    "       target_id = torch.tensor(self.word_to_id[target], dtype=torch.long)\n",
    "       return context_ids, target_id\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "dataset = CBOWDataset(training_data, word_to_id)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=wandb.config.num_workers     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=wandb.config.num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "   def __init__(self, vocab_size, embedding_dim):\n",
    "       super(CBOW, self).__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, inputs):\n",
    "       embed = self.embedding(inputs)\n",
    "       embed = embed.mean(dim=1)\n",
    "       out = self.linear(embed)\n",
    "       probs = F.log_softmax(out, dim=1)\n",
    "       return probs\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(\n",
    "   vocab_size=wandb.config.vocab_size,\n",
    "   embedding_dim=wandb.config.embedding_dim)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=wandb.config.learning_rate,\n",
    "                            weight_decay=wandb.config.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Add evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "   model.eval()\n",
    "   total_loss = 0\n",
    "   with torch.no_grad():\n",
    "      for context, target in test_loader:\n",
    "           context, target = context.to(device), target.to(device)\n",
    "           output = model(context)\n",
    "           loss = criterion(output, target)\n",
    "           total_loss += loss.item()\n",
    "   return total_loss / len(test_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 89224/89224 [04:14<00:00, 349.95it/s, train_loss=6.2445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5:\n",
      "Train Loss: 6.2445\n",
      "Test Loss: 6.1203\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 58\u001b[0m\n\u001b[1;32m     44\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[1;32m     54\u001b[0m })\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Save checkpoint if test loss improved\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_loss \u001b[38;5;241m<\u001b[39m \u001b[43mbest_test_loss\u001b[49m:\n\u001b[1;32m     59\u001b[0m     best_test_loss \u001b[38;5;241m=\u001b[39m test_loss\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew best test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Saving checkpoint...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize best_test_loss with infinity before the training loop\n",
    "best_test_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "epochs_no_improve = 0  # Counter for epochs with no improvement\n",
    "\n",
    "# Modified training loop with progress bar and test loss\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "   model.train()\n",
    "   total_loss = 0\n",
    "  \n",
    "   # Create progress bar for each epoch\n",
    "   progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{wandb.config.num_epochs}')\n",
    "  \n",
    "   for batch_idx, (context, target) in enumerate(progress_bar):\n",
    "       context, target = context.to(device), target.to(device)\n",
    "      \n",
    "       optimizer.zero_grad()\n",
    "       output = model(context)\n",
    "       loss = criterion(output, target)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "      \n",
    "       total_loss += loss.item()\n",
    "       current_loss = total_loss / (batch_idx + 1)\n",
    "      \n",
    "       # Progress bar for both losses\n",
    "       progress_bar.set_postfix({\n",
    "           'train_loss': f'{current_loss:.4f}'\n",
    "       })\n",
    "      \n",
    "       # Log batch metrics\n",
    "       wandb.log({\n",
    "           \"batch_loss\": loss.item(),\n",
    "           \"batch\": batch_idx + epoch * len(train_loader)\n",
    "       })\n",
    "  \n",
    "   # Calculate average training loss\n",
    "   train_loss = total_loss / len(train_loader)\n",
    "  \n",
    "   # Calculate test loss\n",
    "   test_loss = evaluate(model, test_loader, criterion, device)\n",
    "  \n",
    "   # Print epoch summary\n",
    "   print(f'\\nEpoch {epoch+1}/{wandb.config.num_epochs}:')\n",
    "   print(f'Train Loss: {train_loss:.4f}')\n",
    "   print(f'Test Loss: {test_loss:.4f}')\n",
    "  \n",
    "   # Log epoch metrics\n",
    "   wandb.log({\n",
    "       \"epoch\": epoch + 1,\n",
    "       \"train_loss\": train_loss,\n",
    "       \"test_loss\": test_loss,\n",
    "       \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "       \"embedding_dim\": wandb.config.embedding_dim,\n",
    "       \"vocab_size\": wandb.config.vocab_size,\n",
    "       \"batch_size\": wandb.config.batch_size,\n",
    "       \"weight_decay\": wandb.config.weight_decay,\n",
    "       \"num_workers\": wandb.config.num_workers\n",
    "   })\n",
    "\n",
    "\n",
    "   # Save checkpoint if test loss improved\n",
    "   if test_loss < best_test_loss:\n",
    "       best_test_loss = test_loss\n",
    "       print(f\"New best test loss: {test_loss:.4f}. Saving checkpoint...\")\n",
    "      \n",
    "       # Save model checkpoint\n",
    "       checkpoint_path = f\"cbow_best_model_epoch{epoch+1}.pt\"\n",
    "       torch.save({\n",
    "           'epoch': epoch,\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'train_loss': train_loss,\n",
    "           'test_loss': test_loss,\n",
    "           'config': wandb.config,\n",
    "       }, checkpoint_path)\n",
    "      \n",
    "       # Extract the embedding matrix from the model\n",
    "       embedding_weights = model.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "       # Log embedding weights histogram to wandb\n",
    "       wandb.log({\n",
    "           \"embedding_weights\": wandb.Histogram(embedding_weights)\n",
    "       })\n",
    "\n",
    "\n",
    "       # save embeddings as .pt version\n",
    "       torch.save(model.embedding.weight.data.cpu(), f\"./model/embeddings_epoch{epoch+1}.pt\")\n",
    "\n",
    "\n",
    "       # Log to wandb\n",
    "       embedding_path = f\"./model/embeddings_epoch{epoch+1}.pt\"\n",
    "       wandb.save(embedding_path)\n",
    "       print(f\"Embeddings saved to {embedding_path}\")   \n",
    "\n",
    "\n",
    "       # Save to wandb\n",
    "       wandb.save(checkpoint_path)\n",
    "       print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "       \n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered. No improvement in test loss for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_checkpoint_path = \"./model/cbow_final_model.pt\"\n",
    "torch.save({\n",
    "   'epoch': wandb.config.num_epochs,\n",
    "   'model_state_dict': model.state_dict(),\n",
    "   'optimizer_state_dict': optimizer.state_dict(),\n",
    "   'train_loss': train_loss,\n",
    "   'test_loss': test_loss,\n",
    "   'config': wandb.config,\n",
    "}, final_checkpoint_path)\n",
    "wandb.save(final_checkpoint_path)\n",
    "print(f\"\\nTraining completed. Final model saved to {final_checkpoint_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
