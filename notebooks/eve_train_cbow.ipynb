{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "wiki_text_file = \"../data/wiki_text_data.txt\"\n",
    "\n",
    "\n",
    "with open(wiki_text_file, \"r\") as file:\n",
    "   text = file.read()\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "  \n",
    "   # remove punctuation and non alphabetic characters\n",
    "   remove_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
    "   lower_case_words = remove_punctuation.lower()\n",
    "   words = lower_case_words.split(' ')\n",
    "\n",
    "\n",
    "   # print count of words in split_words_by_whitespace\n",
    "   print(f\"Number of words before filtering: {len(words)}\")\n",
    "\n",
    "\n",
    "   # get word counts\n",
    "\n",
    "\n",
    "   top_k = 30000\n",
    "   word_counts = Counter(words)\n",
    "   top_words = dict(word_counts.most_common(top_k))\n",
    "   word_to_id = {word: i for i, word in enumerate(top_words.keys())}\n",
    "   id_to_word = {i: word for i, word in enumerate(top_words.keys())}\n",
    "\n",
    "\n",
    "   # Sum their counts\n",
    "   total_count = sum(count for word, count in top_words.items())\n",
    "\n",
    "\n",
    "   print(f\"Total count of top {top_k} words: {total_count}\")\n",
    "   # Optional: Show what percentage of all words this represents\n",
    "   total_words = sum(word_counts.values())\n",
    "   percentage = (total_count / total_words) * 100\n",
    "   print(f\"This represents {percentage:.2f}% of all words in the corpus\")\n",
    "\n",
    "\n",
    "   # filter corpus to only include words in the tok k words\n",
    "   corpus = [word for word in words if word in top_words]\n",
    "   print(\"corpus length:\", len(corpus))\n",
    "\n",
    "\n",
    "   return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before filtering: 17005208\n",
      "Total count of top 30000 words: 16315126\n",
      "This represents 95.94% of all words in the corpus\n",
      "corpus length: 16315126\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "word_to_id, id_to_word, corpus = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#print(corpus[:100])\n",
    "\n",
    "# Generate the training data from the corpus\n",
    "# The training data looks like a list of tuples,\n",
    "# where each tuple contains a list of context words and the target word (not the IDs)\n",
    "\n",
    "\n",
    "def generate_training_data(corpus):\n",
    "   data = []\n",
    "\n",
    "\n",
    "   # start from index 2 and end 2 positions before the last word\n",
    "   # this ensures we always have 2 words before and after the target word\n",
    "   # for a 5-len sliding window\n",
    "\n",
    "\n",
    "   for i in range(2, len(corpus) - 2):\n",
    "       # Get the context words\n",
    "       # 'i' is the index of the target word\n",
    "       # [i-2:i] gets the two words before the target word\n",
    "       # [i+1:i+3] gets the two words after the target word\n",
    "       context_words = corpus[i-2:i] + corpus[i+1:i+3]\n",
    "      \n",
    "       # Get the target word\n",
    "       target_word = corpus[i]\n",
    "\n",
    "\n",
    "       # Append the tuple to the data list\n",
    "       data.append((context_words, target_word))\n",
    "\n",
    "\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW training data generated\n"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "training_data = generate_training_data(corpus)\n",
    "print(\"CBOW training data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train w2v cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import tqdm\n",
    "import wandb\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create your dataset class and cbow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "GPU Memory: 10.39 GB\n"
     ]
    }
   ],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "   def __init__(self, data, word_to_id):\n",
    "       self.data = data\n",
    "       self.word_to_id = word_to_id\n",
    "\n",
    "\n",
    "   # overriding the __len__ method to tell PyTorch how many samples you have\n",
    "   def __len__(self):\n",
    "       return len(self.data)\n",
    "\n",
    "\n",
    "   # overriding the __getitem__ method\n",
    "   # to tell PyTorch how to retrieve a specific sample and convert it to the format your model expects\n",
    "   def __getitem__(self, idx):\n",
    "       context, target = self.data[idx]\n",
    "       context_ids = torch.tensor([self.word_to_id[word] for word in context], dtype=torch.long)\n",
    "       target_id = torch.tensor(self.word_to_id[target], dtype=torch.long)\n",
    "       return context_ids, target_id\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "   def __init__(self, vocab_size, embedding_dim):\n",
    "       super(CBOW, self).__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "       self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, inputs):\n",
    "       embed = self.embedding(inputs)\n",
    "       embed = embed.mean(dim=1)\n",
    "       out = self.linear(embed)\n",
    "       probs = F.log_softmax(out, dim=1)\n",
    "       return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load everything in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize settings\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "   print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "   # Enable cuDNN auto-tuner\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "\n",
    "dataset = CBOWDataset(training_data, word_to_id)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Initialize wandb with your configuration\n",
    "wandb.init(\n",
    "   project=\"cbow-wiki\",\n",
    "   name=f\"{timestamp}\",\n",
    "   config={\n",
    "       # Model parameters\n",
    "       \"embedding_dim\": 200,\n",
    "       \"vocab_size\": 30000,\n",
    "      \n",
    "       # Training parameters\n",
    "       \"batch_size\": 128,\n",
    "       \"learning_rate\": 0.001,\n",
    "       \"num_epochs\": 5,\n",
    "       \"train_split\": 0.7,\n",
    "      \n",
    "       # Optimizer parameters\n",
    "       \"weight_decay\": 1e-5,\n",
    "      \n",
    "       # DataLoader parameters\n",
    "       \"num_workers\": 4,\n",
    "      \n",
    "       # Architecture details\n",
    "       \"model_type\": \"CBOW\",\n",
    "       \"context_size\": 4  # 2 words before + 2 words after\n",
    "   }\n",
    ")\n",
    "\n",
    "# Then use the config values throughout your code\n",
    "EMBEDDING_DIM = wandb.config.embedding_dim\n",
    "BATCH_SIZE = wandb.config.batch_size\n",
    "LEARNING_RATE = wandb.config.learning_rate\n",
    "NUM_EPOCHS = wandb.config.num_epochs\n",
    "TRAIN_SPLIT = wandb.config.train_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with GPU pinning\n",
    "train_loader = DataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=True,\n",
    "   pin_memory=True,  # Enable pinning for faster GPU transfer\n",
    "   num_workers=wandb.config.num_workers     # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "   batch_size=wandb.config.batch_size,\n",
    "   shuffle=False,\n",
    "   pin_memory=True,\n",
    "   num_workers=wandb.config.num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(\n",
    "   vocab_size=wandb.config.vocab_size,\n",
    "   embedding_dim=wandb.config.embedding_dim)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=wandb.config.learning_rate,\n",
    "                            weight_decay=wandb.config.weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Add evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "   model.eval()\n",
    "   total_loss = 0\n",
    "   with torch.no_grad():\n",
    "      for context, target in test_loader:\n",
    "           context, target = context.to(device), target.to(device)\n",
    "           output = model(context)\n",
    "           loss = criterion(output, target)\n",
    "           total_loss += loss.item()\n",
    "   return total_loss / len(test_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 6.0939, Test Loss: 6.0956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Loss: 6.0901, Test Loss: 6.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Loss: 6.0878, Test Loss: 6.0912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Loss: 6.0860, Test Loss: 6.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Loss: 6.0847, Test Loss: 6.0885\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>▅▄▆▄▇▃▇▆█▅▅▄▃▄▆▁▂▃▅▃▇█▄▄▆▄▄▅▄▃▇▅▃▅▂▆▄▄▅▃</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁</td></tr><tr><td>embedding_dim</td><td>▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>▃▆█▇▅▅▄▄▄▇▃▅▄▄▅▃▄▄▂▅▄▂▂▅▁▄▁▅▅▅▄▃▆▄▆▂▂▇▁▆</td></tr><tr><td>test_loss</td><td>█▅▄▂▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>5.33394</td></tr><tr><td>batch_size</td><td>128</td></tr><tr><td>embedding_dim</td><td>200</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>6.39345</td></tr><tr><td>test_loss</td><td>6.08852</td></tr><tr><td>train_loss</td><td>6.08467</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025_04_17__15_28_43</strong> at: <a href='https://wandb.ai/bryars-bryars/cbow-wiki/runs/ypcolpun' target=\"_blank\">https://wandb.ai/bryars-bryars/cbow-wiki/runs/ypcolpun</a><br> View project at: <a href='https://wandb.ai/bryars-bryars/cbow-wiki' target=\"_blank\">https://wandb.ai/bryars-bryars/cbow-wiki</a><br>Synced 5 W&B file(s), 0 media file(s), 24 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_152848-ypcolpun/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simplified training loop\n",
    "# Training Loop\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False)\n",
    "    for inputs, targets in progress:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        wandb.log({'batch_loss': loss.item()})\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "    # Log epoch metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'test_loss': avg_test_loss,\n",
    "        'embedding_dim': wandb.config.embedding_dim,\n",
    "        'batch_size': wandb.config.batch_size\n",
    "    })\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{wandb.config.num_epochs}: Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # Save model checkpoint after every epoch\n",
    "    checkpoint_path = f\"../model/cbow_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    model_artifact = wandb.Artifact('model-weights', type='model')\n",
    "    model_artifact.add_file(checkpoint_path)\n",
    "    wandb.log_artifact(model_artifact)\n",
    "\n",
    "    # Save embeddings separately\n",
    "    embedding_weights = model.embedding.weight.data.cpu()\n",
    "    embedding_path = f\"../model/embeddings_epoch{epoch+1}_{timestamp}.pt\"\n",
    "    torch.save(embedding_weights, embedding_path)\n",
    "    embedding_artifact = wandb.Artifact('embeddings', type='embeddings')\n",
    "    embedding_artifact.add_file(embedding_path)\n",
    "    wandb.log_artifact(embedding_artifact)\n",
    "\n",
    "# Finish Wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test of similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similar_words(query_word, word_to_ix, ix_to_word, embeddings, top_k=5):\n",
    "    if query_word not in word_to_ix:\n",
    "        print(f\"'{query_word}' not in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    query_idx = word_to_ix[query_word]\n",
    "    query_embedding = embeddings[query_idx]\n",
    "\n",
    "    # Compute cosine similarity with all embeddings\n",
    "    similarities = F.cosine_similarity(query_embedding.unsqueeze(0), embeddings)\n",
    "\n",
    "    # Get top_k most similar (excluding the query word itself)\n",
    "    top_indices = similarities.argsort(descending=True)[1:top_k+1]\n",
    "    similar_words = [(ix_to_word[idx.item()], similarities[idx].item()) for idx in top_indices]\n",
    "\n",
    "    return similar_words\n",
    "\n",
    "# Normalize embeddings\n",
    "embedding_weights = model.embedding.weight.data  # shape: [vocab_size, embedding_dim]\n",
    "norms = embedding_weights.norm(dim=1, keepdim=True)\n",
    "normalized_embeddings = embedding_weights / norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to castle:\n",
      "castle (1.0000)\n",
      "nagoya (0.5787)\n",
      "lambeth (0.5698)\n",
      "expense (0.5653)\n",
      "beverley (0.5635)\n",
      "communicates (0.5601)\n",
      "palace (0.5586)\n",
      "yorktown (0.5575)\n",
      "usc (0.5570)\n",
      "knossos (0.5567)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "word = \"castle\"\n",
    "similar = get_similar_words(word, word_to_id, id_to_word, normalized_embeddings, top_k=10)\n",
    "print(f\"Words similar to {word}:\")\n",
    "for word, score in similar:\n",
    "    print(f\"{word} ({score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
